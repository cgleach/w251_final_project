{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/home/wce/clsadmin\n"
                }
            ], 
            "source": "%%sh\npwd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%%sh\npip install python-dotenv"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%%sh\npip install plotly\npip install pyspark\npip install geopandas==0.3.0\npip install pyshp==1.2.10\npip install shapely==1.6.3"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Import packages\nimport plotly as plt\nimport pandas as pd\nimport dotenv as dt\nfrom pyspark.sql import SparkSession"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%%sh\npip freeze"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%%sh\nls data"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# reading in data\ndf = spark.read.csv(\"/data/ca/*\", header=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.head(10)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.select(\"LON\").show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# creating a dataframe of just longitudes and latitudes, potentially for future use\nlonlat = df.select(df[\"LAT\"], df[\"LON\"])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# this his how we need to feed the \nimport requests\nresponse = requests.get(\"https://geo.fcc.gov/api/census/block/find?latitude=33.786405&longitude=-86.9329571&format=xml\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# json format is returned, but successful request to api\nprint(response.content)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# simple proof of concept to show that we cna feed LON and LAT into the below\nLON = -118.219827\nLAT = 34.0424273\n\n\ntest = requests.get(\"https://geo.fcc.gov/api/census/block/find?latitude=%s&longitude=%s&format=xml\" % (LAT,LON))\nprint(test.content)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.types import *\nfrom pyspark.sql.functions import udf, struct\n\n#df_test = pd.DataFrame(df.head(10), columns=df.columns)\n\nget_fips = udf(lambda x, y: \n                   requests.get(\"https://geo.fcc.gov/api/census/block/find?latitude=%s&longitude=%s&format=xml\" % (str(x),str(y))), IntegerType())\n\n\n\n#k = df.withColumn(\"FIPS\",get_fips(df[\"LAT\"],df[\"LON\"] ))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "get_fips = udf(lambda x: \"https://geo.fcc.gov/api/census/block/find?latitude=%s&longitude=%s&format=xml\" % (x.LAT,x.LON), IntegerType())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "k = df.rdd.map(lambda x: requests.get(\"https://geo.fcc.gov/api/census/block/find?latitude=%s&longitude=%s&format=xml\" % (x.LAT,x.LON)))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "test_df = df.withColumn(\"FIPS\",get_fips(df[\"LAT\"], df[\"LON\"))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.sql.functions import udf\n\n# Use udf to define a row-at-a-time udf\n@udf('double')\n# Input/output are both a single double value\ndef plus_one(v):\n      return v + 1\n\ndf.withColumn('LONPLUS', plus_one(df.LON))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.select('LONPLUS')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2.7 with Spark 2.3 (YARN Client Mode)", 
            "name": "python2-spark23", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.13", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}