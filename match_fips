# Packages needed for notebook
import pandas as pd
import math
from pyspark.sql.functions import udf, struct
from pyspark.sql.types import *
from pyspark.sql.functions import lit
from functools import reduce  # For Python 3.x
from pyspark.sql import DataFrame
from pyspark.sql import SparkSession
from pyspark.sql.types import *


def calc_fip(row, fip_dict):
    try:
        fips =  fip_dict[row.infer_zip]
        return fips
    except:
        return None       
        
def udf_fip(fip_dict):
    return udf(lambda l: calc_zip(l,fip_dict))

def run():
    # define state list
    statesCap = ["AZ", "AL", "AR", "CA", "CO", "CT", "DC", "DE", 
                 "FL", "GA", "HI", "ID", "IL", "IN", "IA", "KS", "KY", 
                 "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", 
                 "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", 
                 "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", 
                 "VT", "VA", "WA", "WV", "WI", "WY"] 
    states = [x.lower() for x in statesCap]
    fips_map = pd.read_csv('/home/wce/clsadmin/data/.csv')
    for state in states:
        pathway = '/states_with_zip/%s/*' %state
        df = sc.read.csv(pathway, header=True, schema = StructType([StructField("LON",DoubleType()),StructField("LAT",DoubleType()),StructField("Number",IntegerType()),StructField("Street",StringType()),StructField("Unit",IntegerType()),StructField("City",StringType()),StructField("District",StringType()),StructField("POSTCODE",StringType()),StructField("ID",IntegerType()),StructField("Hash",StringType()),StructField("State", StringType()),StructField("null_count",IntegerType())])).persist()
        df = df.rdd.repartition(256).toDF(sampleRatio=1.0)
        fip_codes = fips_map[fips_map['State']==state.upper()]
        fip_codes = fip_codes[['ZIP','STATE','STCOUNTYFP']]
        fip_codes = fip_codes.drop_duplicates()

        fip_dict = {}
        for index,row in fip_codes.iterrows():
            fip_dict[row['ZIP']] = row['STCOUNTYFP']
    
        output_path = '/final_data/%s' %state
        print(output_path)
        final_df = df.withColumn("fips", udf_fip(fip_dict)(struct(['infer_zip]))).persist()
        final_df.show(100)
        #final_df.write.csv(output_path, header=True, nullValue='')
    return


if __name__ == "__main__":
    sc = SparkSession.builder.appName('state_zips').getOrCreate()
    s_context = sc.sparkContext
    run()
    print("run succesful")
